{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb011026d933971",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50375b26fe99c498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:36:06.938822Z",
     "start_time": "2025-12-02T19:36:06.932252Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = \"models/transformer_6.8M.pt\"\n",
    "checkpoint = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c2b4b5f197ab65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:35:31.286642Z",
     "start_time": "2025-12-02T19:35:31.260106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:   4000\n",
      "D_Model:      256\n",
      "Layers:       6\n",
      "Heads:        8\n",
      "Head Dim:     32\n",
      "Seq Len:      128\n",
      "FF Hidden:    1024\n",
      "Dropout:      0.1\n",
      "num_params:   6.819.232\n"
     ]
    }
   ],
   "source": [
    "from src.enums import CheckpointEnum\n",
    "\n",
    "print(f\"Vocab Size:   {checkpoint[CheckpointEnum.VOCAB_SIZE]}\")\n",
    "print(f\"D_Model:      {checkpoint[CheckpointEnum.D_MODEL]}\")\n",
    "print(f\"Layers:       {checkpoint[CheckpointEnum.NUM_BLOCKS]}\")\n",
    "print(f\"Heads:        {checkpoint[CheckpointEnum.NUM_HEADS]}\")\n",
    "print(f\"Head Dim:     {checkpoint[CheckpointEnum.D_MODEL] // checkpoint[CheckpointEnum.NUM_HEADS]}\")\n",
    "print(f\"Seq Len:      {checkpoint[CheckpointEnum.SEQ_LEN]}\")\n",
    "print(f\"FF Hidden:    {checkpoint[CheckpointEnum.FF_HIDDEN_DIM]}\")\n",
    "print(f\"Dropout:      {checkpoint[CheckpointEnum.DROPOUT]}\")\n",
    "print(f\"num_params:   {checkpoint['num_params']:,}\".replace(\",\", \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ad1bd66cec55d",
   "metadata": {},
   "source": [
    "# Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb0275f5606cbb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:18:18.055509Z",
     "start_time": "2025-12-03T08:18:16.906880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BPE hugging face tokenizer - vocab size: 4000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.utils import load_bpe_hugging_face_tokenizer\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "TOKENIZER_DIR = BASE_DIR / \"tokenizer\"\n",
    "path = TOKENIZER_DIR / \"bpe_hugging_face_tokenizer.json\"\n",
    "\n",
    "\n",
    "tokenizer = load_bpe_hugging_face_tokenizer(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aceb481a4dc4fc",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3752139086a4cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:18:43.368326Z",
     "start_time": "2025-12-03T08:18:43.363709Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "path = BASE_DIR / \"data\" / \"letterboxd_filtered.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c5b70753473fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:18:45.807907Z",
     "start_time": "2025-12-03T08:18:44.674682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 564.702\n"
     ]
    }
   ],
   "source": [
    "from src.utils.data_loader import read_file_only_reviews\n",
    "\n",
    "text = read_file_only_reviews(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a15fe483a26dea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:20:41.455771Z",
     "start_time": "2025-12-02T20:20:41.448318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as soon as this film ended i went online and enlisted in the US army. no child will ever suffer like this again on my watch\n",
      "Come and See is a film I find almost impossible to review. Describing watching a film as an 'experience' often detracts from the quality of the piece, but going by the profound effect the film had on me I really feel no other word can do it justice.\n",
      "An apocalyptic nightmare of pure brutalizing evil shot and sequenced in intimate, hyperreal historical detail. Unbearable extreme close-ups of its witnesses and beautiful roaming camera moves (ensuring we see even what the characters maybe miss) that in tandem form a series of vivid, unsparing, and surreal vignettes around the psychological experience of unimaginably barbaric horror being made tangible. You can practically smell the mud and fire and corpses, and just like the young boy in this film I too feel like I just aged 100 years in 2 hours.\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(text[1])\n",
    "print(text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76516856c677f2",
   "metadata": {},
   "source": [
    "## analysing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343ce9b0bd3a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:41:05.898764Z",
     "start_time": "2025-12-02T20:41:05.622033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('as soon as this film ended i went online and enlisted in the US army. no child will ever suffer like this again on my watch',\n",
       "  1),\n",
       " (\"Come and See is a film I find almost impossible to review. Describing watching a film as an 'experience' often detracts from the quality of the piece, but going by the profound effect the film had on me I really feel no other word can do it justice.\",\n",
       "  1),\n",
       " ('An apocalyptic nightmare of pure brutalizing evil shot and sequenced in intimate, hyperreal historical detail. Unbearable extreme close-ups of its witnesses and beautiful roaming camera moves (ensuring we see even what the characters maybe miss) that in tandem form a series of vivid, unsparing, and surreal vignettes around the psychological experience of unimaginably barbaric horror being made tangible. You can practically smell the mud and fire and corpses, and just like the young boy in this film I too feel like I just aged 100 years in 2 hours.',\n",
       "  1),\n",
       " ('One of those where you finish it and kinda just sit there in disbelief that this is a movie that exists. Sheesh, what a monster.',\n",
       "  1),\n",
       " ('The astonishing thing aboutSeven Samuraiis that it seems far too modern for a film made sixty years ago. As much as I love black-and-white films, it is hard to deny that some films, no matter how good they are, begin to seem antiquated when compared with modern films in their genre.',\n",
       "  1),\n",
       " (\"Had the pleasure of watching this film for the first time on 35mm thanks to The Cinematheque in Vancouver. It was the first film in the series High Art Films Made from Pop Art Sources curated by Donald Brackett. Brackett intro'd the film.\",\n",
       "  1),\n",
       " ('Everyone is out for my man Mifune when all he wants to do is make high-quality shoes',\n",
       "  1),\n",
       " ('Why is it that I still dread watching \"classics\"? I\\'m talking about those foreign, black and white movies that are whispered about only in the halls of letterboxd. Why do I put off watching them for months, and finally force myself to watch them only through the guise of a challenge? Was Rashomon not enough? Did the magnificence of Citizen Kane not convince me that hyperbole has no sway when a masterpiece is on screen? Did I not finally sit down, ready to \"endure\" Tokyo Story, only to slap my forehead and groan, \"Never again, you idiot.\"',\n",
       "  1),\n",
       " (\"Brilliant. Riveting exploration of mortality and honor. Timeless, impeccable, and hypnotic. I find myself looking forward to future viewings, and I'm sure there will be many.\",\n",
       "  1),\n",
       " ('God damn what an impeccable film. Not dated in the least, if you tried to tell this same story now, 60 years later, this would still be the best way to tell it, shot for shot, line for line.',\n",
       "  1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(text).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9166ec66277fb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:41:49.841795Z",
     "start_time": "2025-12-02T20:41:49.680238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for occurrences of: 'This review may contain spoilers.I can handle the truth.'\n",
      "\n",
      "count: 0\n"
     ]
    }
   ],
   "source": [
    "target_text = \"This review may contain spoilers.I can handle the truth.\"\n",
    "\n",
    "print(f\"Searching for occurrences of: '{target_text}'\\n\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index, review in enumerate(text):\n",
    "    if review == target_text:\n",
    "        print(f\"Found at Index {index}: {review}\")\n",
    "        count += 1\n",
    "\n",
    "    if count >= 5:\n",
    "        break\n",
    "\n",
    "print(f\"count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633e35b893fd34d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:42:05.451798Z",
     "start_time": "2025-12-02T20:41:54.945021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 1.252.140\n",
      "a: 841.196\n",
      "and: 721.303\n",
      "of: 671.811\n",
      "to: 546.293\n",
      "i: 457.224\n",
      "it: 446.417\n",
      "in: 382.707\n",
      "is: 367.226\n",
      "this: 366.725\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "full_text = \" \".join(text)\n",
    "full_text = full_text.lower()\n",
    "\n",
    "words = re.findall(r\"\\w+\", full_text)\n",
    "\n",
    "word_counts = Counter(words)\n",
    "\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count:,}\".replace(\",\", \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_review_lengths(texts: list[str]) -> list[int]:\n",
    "    lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "\n",
    "        lengths.append(len(text.split()))\n",
    "\n",
    "    return lengths\n",
    "\n",
    "\n",
    "lengths = get_token_lengths(text, tokenizer)\n",
    "\n",
    "df_lengths = pd.Series(lengths)\n",
    "\n",
    "print(df_lengths.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b529d4b930195",
   "metadata": {},
   "source": [
    "## analysing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e7524ca9e5e56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:19:25.396845Z",
     "start_time": "2025-12-03T08:19:09.634260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    564702.000000\n",
      "mean         69.312297\n",
      "std          47.153609\n",
      "min          15.000000\n",
      "25%          34.000000\n",
      "50%          54.000000\n",
      "75%          91.000000\n",
      "max        1192.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_token_lengths(texts: list[str], tokenizer, batch_size: int = 1000) -> list[int]:\n",
    "    lengths = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        encoded_batch = tokenizer.encode_batch(batch)\n",
    "\n",
    "        for encoded in encoded_batch:\n",
    "            lengths.append(len(encoded.ids))\n",
    "\n",
    "    return lengths\n",
    "\n",
    "\n",
    "lengths = get_token_lengths(text, tokenizer)\n",
    "\n",
    "df_lengths = pd.Series(lengths)\n",
    "\n",
    "print(df_lengths.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-trufiti-group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
