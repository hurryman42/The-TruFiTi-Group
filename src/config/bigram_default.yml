model:
  type: bigram
  d_model: 128

tokenizer:
  type: bpe_hugging_face

training:
  batch_size: 64
  seq_len: 128
  learning_rate: 0.001
  max_iters: 1000
  eval_interval: 200
  eval_iters: 50

data:
  train_size: 0.8
  val_size: 0.1
  test_size: 0.1
  file: letterboxd_filtered_llm.jsonl
  level: 2
  seed: 42
