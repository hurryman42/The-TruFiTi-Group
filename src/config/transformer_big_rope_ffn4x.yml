model:
  type: transformer
  d_model: 256
  num_heads: 8
  num_blocks: 8
  seq_len: 256
  dropout: 0.1
  use_rope: True

tokenizer:
  type: bpe_hugging_face

training:
  batch_size: 16
  learning_rate: 1e-4
  weight_decay: 0.01
  max_iters: 20000
  eval_interval: 500
  eval_iters: 50
  warmup_iters: 500

data:
  train_size: 0.8
  val_size: 0.1
  test_size: 0.1
  file: letterboxd_filtered_llm_filtered.jsonl
  level: 2
  seed: 42
