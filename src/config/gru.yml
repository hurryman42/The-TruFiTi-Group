  model:
    type: gru
    input_size: 64
    hidden_size: 256
    num_layers: 2
    seq_len: 128
    dropout: 0.2
  tokenizer:
    type: bpe_hugging_face

  training:
    batch_size: 64
    learning_rate: 3e-4
    weight_decay: 0.01
    max_iters: 25000
    eval_interval: 500
    eval_iters: 50
    warmup_iters: 200

  data:
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    file: letterboxd_filtered_llm.jsonl
    level: 2
    seed: 42